#!/usr/bin/env bash

# haha cache go brrr

# a very silly, very shitty little DNS cache warmup tool born of sleep
# deprivation, far too much coffee, and a poor moral compass

# are you marginally inconvenienced by ~200ms cold recursive lookups?
# do you hate root servers?
# have you got too much cpu on your hands?
# is you relationship with your internet service provider too good?
# ...then step right up, this tool is the tool for you!


export LC_ALL=C

# if you supply an alternate csv through domain_list_url you may need
# to adjust this value
csv_column="3"

# more information about the following list is available at
# https://majestic.com/reports/majestic-million
domain_list_url="https://downloads.majestic.com/majestic_million.csv"

# it would be thoroughly irresponsible to point this at anything you
# don't own
resolver_address="127.0.0.1"

# resolver listening port
# using the unbound listening port by default
resolver_port="5335"

# total domains
# the number of unique domains we'll query for, preferably divisible
# by four
# 1000 domains is a nice "safe" number
total_domains="1000"

# don't use parallel by default
use_parallel=""


# obnoxious ascii header
# every script on the internet needs a large, preferably coloured, ascii banner
# this one is no exception
obnoxious_header() {
	echo -e "\e[31;1m    _         _                      _                     _                \e[0m"
	echo -e "\e[31;1m   | |_  __ _| |_  __ _   __ __ _ __| |_  ___   __ _ ___  | |__ _ _ _ _ _ _ \e[0m"
	echo -e "\e[31;1m   | ' \/ _\` | ' \/ _\` | / _/ _\` / _| ' \/ -_) / _\` / _ \ | '_ \ '_| '_| '_|\e[0m"
	echo -e "\e[31;1m   |_||_\__,_|_||_\__,_| \__\__,_\__|_||_\___| \__, \___/ |_.__/_| |_| |_|  \e[0m"
	echo -e "\e[31;1m                                               |___/                        \e[0m\n"
	echo -e "\e[31;1m             a very silly, very shitty little DNS cache warmup tool             \e[0m\n"
}

obnoxious_header


# perform cleanup first
# previous aborted runs might mess us up, make sure we have a clean
# slate to work with
cleanup


# download the top domains list
download_list() {
	if [ ! -f /tmp/domains_raw ]; then
		echo "downloading the top domains list ..."
		echo -e "\tthis may take some time"
		wget -nv -O /tmp/domains_raw ${domain_list_url}
		echo -e "\t- done\n"
	else
		echo -e "\t- top domains list already exists, skipping ...\n"
	fi
}

download_list


# prepare domains
prepare_domains() {
	echo "preparing the top ${total_domains} domains ..."
	# strip out the top domain entries
	let cut_limit="${total_domains} + 1"
	sed -n -e "2,${cut_limit} p" -e "${cut_limit} q" /tmp/domains_raw | cat >> /tmp/top_domains_raw
	echo -e "\t- done\n"
	echo "filtering the domain column from the scv ..."
	# filter out the domain column from the csv
	cut -d , -f ${csv_column} /tmp/top_domains_raw | cat >> /tmp/domain_list
	echo -e "\t- done\n"
	echo "building the master list of dig commands ..."
	# build the list of commands that we'll send to our resolver
	sed -e "s/.*/dig +short +time=10 +tries=1 & @${resolver_address} -p ${resolver_port}/" /tmp/domain_list | cat >> /tmp/dig_commands_main
	echo -e "\t- done\n"
}

prepare_domains


# split the dig command list into smaller batches
split_dig_commands() {
	echo "splitting dig commands ..."
	# split into batches
	let split_limit="${total_domains / 4}"
	split -d -l ${split_limit} /tmp/dig_commands_main /tmp/dig_commands_split
	echo -e "\t- done\n"
	echo "making split dig command lists exectuable ..."
	# make them executable
	chmod +x /tmp/dig_commands_split**
	echo -e "\t- done\n"
}


# lets get parallel
run_parallel_dig_commands() {
	echo "running dig commands in parallel ..."
	# use gnu parallel to do exactly what it sounds like
	parallel -u ::: /tmp/dig_commands_split00 /tmp/dig_commands_split01 /tmp/dig_commands_split02 /tmp/dig_commands_split03
	echo -e "\t- done\n"
}


# we won't use gnu-parallel by default
if [ "${use_parallel}" = "yes" ]; then
	split_dig_commands
	run_parallel_dig_commands
else
	echo "making master dig command list exectuable ..."
	# make them executable
	chmod +x /tmp/dig_commands_main
	echo -e "\t- done\n"
	echo "running dig commands ..."
	# run dig commands
	/tmp/dig_commands_main
	echo -e "\t- done\n"
fi


# custom domains
# the user may want to supply their own list of domains to lookup
custom_domains() {
	if [ -f /tmp/custom_domains ]; then
		echo "custom_domains list found ...\n"
		echo "building custom list of dig commands ..."
		# build the list of commands that we'll send to our resolver
		sed -e "s/.*/dig +short +time=10 +tries=1 & @${resolver_address} -p ${resolver_port}/" /tmp/custom_domains | cat >> /tmp/dig_commands_custom
		echo -e "\t- done\n"
		echo "making custom dig command list exectuable ..."
		# make them executable
		chmod +x /tmp/dig_commands_custom
		echo -e "\t- done\n"
		echo "running custom dig commands ..."
		# run custom dig commands
		/tmp/dig_commands_custom
		echo -e "\t- done\n"
	else
		echo -e "\t- custom domains list not found, skipping ...\n"
	fi
}

custom_domains


# cleanup
cleanup() {
	echo "removing unneeded files ..."
	# keep the raw top domains csv around for subsequent runs, at least as long as it's preserved in /tmp for naturally
	#rm /tmp/domains_raw
	rm /tmp/top_domains_raw
	rm /tmp/domain_list
	rm /tmp/dig_commands_main
	rm /tmp/dig_commands_split*
	echo -e "\t- done\n"
}

cleanup


echo -e "\e[31;1m                               haha cache go brrr                               \e[0m\n"
